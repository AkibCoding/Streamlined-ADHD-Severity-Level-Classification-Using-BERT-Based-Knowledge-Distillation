{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"Peraboom/LastBERT\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"Peraboom/LastBERT\")\n\n# Create a pipeline for question answering\nqa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n\n# Define context and question\ncontext = \"\"\"Hugging Face is a technology company known for developing tools and models for natural language processing (NLP). \nThey provide open-source models, libraries, and APIs that facilitate NLP applications.\"\"\"\nquestion = \"What does Hugging Face develop?\"\n\n# Get the answer\nresponse = qa_pipeline(question=question, context=context)\n\n# Print the result\nprint(f\"Answer: {response['answer']}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-16T02:28:27.019317Z","iopub.execute_input":"2024-10-16T02:28:27.020048Z","iopub.status.idle":"2024-10-16T02:28:27.582082Z","shell.execute_reply.started":"2024-10-16T02:28:27.020005Z","shell.execute_reply":"2024-10-16T02:28:27.581025Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at Peraboom/LastBERT and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Answer: company known for developing tools and models\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n\n# Load the tokenizer and model for BERT-base\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n\n# Create a question-answering pipeline\nqa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n\n# Define context and question\ncontext = \"\"\"Hugging Face is a technology company known for developing tools and models for natural language processing (NLP). \nThey provide open-source models, libraries, and APIs that facilitate NLP applications.\"\"\"\nquestion = \"What does Hugging Face develop?\"\n\n# Get the answer\nresponse = qa_pipeline(question=question, context=context)\n\n# Print the result\nprint(f\"Answer: {response['answer']}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T02:27:52.480632Z","iopub.execute_input":"2024-10-16T02:27:52.481053Z","iopub.status.idle":"2024-10-16T02:27:53.214775Z","shell.execute_reply.started":"2024-10-16T02:27:52.481012Z","shell.execute_reply":"2024-10-16T02:27:53.213861Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Answer: (NLP). \nThey provide open-source models, libraries,\n","output_type":"stream"}]},{"cell_type":"code","source":"# Install necessary libraries\n!pip install transformers datasets scikit-learn\n\n# Import required libraries\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef\n\n# Load the CoLA dataset\ndataset = load_dataset('glue', 'cola')\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"Peraboom/LastBERT\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Peraboom/LastBERT\", num_labels=2)\n\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples['sentence'], padding='max_length', truncation=True)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n\n# Define the metrics for evaluation\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(-1)\n    \n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions)\n    precision = precision_score(labels, predictions)\n    recall = recall_score(labels, predictions)\n    mcc = matthews_corrcoef(labels, predictions)\n    \n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall,\n        'mcc': mcc\n    }\n\n# Set training arguments with advanced techniques and early stopping\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",        # Evaluate every epoch\n    save_strategy=\"epoch\",              # Save every epoch (matching evaluation strategy)\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=10,  # Max epochs\n    weight_decay=0.01,\n    gradient_accumulation_steps=4,\n    logging_dir='./logs',\n    logging_steps=10,       # Log every 10 steps\n    save_total_limit=2,     # Limit saved models to 2\n    load_best_model_at_end=True,  # Load best model at the end\n    metric_for_best_model='mcc',  # Track MCC for the best model\n    greater_is_better=True\n)\n\n# Initialize the Trainer with Early Stopping\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation'],\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Early stopping if no improvement after 3 epochs\n)\n\n# Train the model with early stopping\ntrainer.train()\n\n# Evaluate the model at the end\neval_results = trainer.evaluate()\nprint(f\"Final Evaluation results: {eval_results}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T04:01:33.795118Z","iopub.execute_input":"2024-10-17T04:01:33.795988Z","iopub.status.idle":"2024-10-17T04:20:38.340351Z","shell.execute_reply.started":"2024-10-17T04:01:33.795947Z","shell.execute_reply":"2024-10-17T04:20:38.339440Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1043 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12a18f24af834958945731406a770335"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112884233333236, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88602d5d1df84cd889eed755e84e55a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241017_040222-kw6xrh4u</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/armyant/huggingface/runs/kw6xrh4u' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/armyant/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/armyant/huggingface' target=\"_blank\">https://wandb.ai/armyant/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/armyant/huggingface/runs/kw6xrh4u' target=\"_blank\">https://wandb.ai/armyant/huggingface/runs/kw6xrh4u</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1330' max='1330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1330/1330 18:07, Epoch 9/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Mcc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.607900</td>\n      <td>0.616227</td>\n      <td>0.691275</td>\n      <td>0.817460</td>\n      <td>0.691275</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.600100</td>\n      <td>0.615728</td>\n      <td>0.691275</td>\n      <td>0.817460</td>\n      <td>0.691275</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.590400</td>\n      <td>0.623912</td>\n      <td>0.659636</td>\n      <td>0.781807</td>\n      <td>0.701987</td>\n      <td>0.882108</td>\n      <td>0.059627</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.549500</td>\n      <td>0.641027</td>\n      <td>0.675935</td>\n      <td>0.796386</td>\n      <td>0.703940</td>\n      <td>0.916782</td>\n      <td>0.082379</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.522900</td>\n      <td>0.667434</td>\n      <td>0.651007</td>\n      <td>0.762402</td>\n      <td>0.720099</td>\n      <td>0.809986</td>\n      <td>0.116655</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.528100</td>\n      <td>0.671477</td>\n      <td>0.662512</td>\n      <td>0.775223</td>\n      <td>0.718343</td>\n      <td>0.841886</td>\n      <td>0.121043</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.474600</td>\n      <td>0.691352</td>\n      <td>0.657718</td>\n      <td>0.770122</td>\n      <td>0.718750</td>\n      <td>0.829404</td>\n      <td>0.118098</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.462400</td>\n      <td>0.695075</td>\n      <td>0.661553</td>\n      <td>0.774152</td>\n      <td>0.718527</td>\n      <td>0.839112</td>\n      <td>0.120739</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [66/66 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Final Evaluation results: {'eval_loss': 0.6714769005775452, 'eval_accuracy': 0.6625119846596357, 'eval_f1': 0.7752234993614305, 'eval_precision': 0.7183431952662722, 'eval_recall': 0.841886269070735, 'eval_mcc': 0.12104339837473771, 'eval_runtime': 3.7087, 'eval_samples_per_second': 281.228, 'eval_steps_per_second': 17.796, 'epoch': 9.94392523364486}\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers datasets scikit-learn nltk\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T05:39:53.560176Z","iopub.execute_input":"2024-10-17T05:39:53.561049Z","iopub.status.idle":"2024-10-17T05:40:05.259126Z","shell.execute_reply.started":"2024-10-17T05:39:53.561009Z","shell.execute_reply":"2024-10-17T05:40:05.257999Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import required libraries\nimport random\nimport pandas as pd\nfrom datasets import load_dataset, Dataset, concatenate_datasets, ClassLabel\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef\nfrom torch import nn\nimport torch\n\n# Load the CoLA dataset\ndataset = load_dataset('glue', 'cola')\n\n# Class weighting: Calculate weights based on class distribution\ndef get_class_weights(dataset):\n    labels = [sample['label'] for sample in dataset['train']]\n    class_0_count = labels.count(0)\n    class_1_count = labels.count(1)\n    \n    # Assign higher weight to the minority class (unacceptable sentences)\n    total_samples = len(labels)\n    class_weights = [total_samples / class_0_count, total_samples / class_1_count]\n    return torch.tensor(class_weights).to('cuda')\n\nclass_weights = get_class_weights(dataset)\n\n# Simple Synonym Replacement using a dictionary (without external libraries)\nsynonym_dict = {\n    \"good\": [\"great\", \"excellent\", \"fine\"],\n    \"bad\": [\"terrible\", \"awful\", \"poor\"],\n    \"happy\": [\"joyful\", \"content\", \"pleased\"],\n    \"sad\": [\"unhappy\", \"sorrowful\", \"down\"],\n    # Add more synonym mappings here for real-world cases\n}\n\ndef replace_with_synonym(word):\n    if word in synonym_dict:\n        return random.choice(synonym_dict[word])\n    return word\n\ndef augment_text(dataset):\n    augmented_sentences = []\n    for sample in dataset['train']:\n        words = sample['sentence'].split()\n        augmented_words = [replace_with_synonym(word) for word in words]\n        augmented_sentence = \" \".join(augmented_words)\n        augmented_sentences.append({\n            'sentence': augmented_sentence,\n            'label': sample['label']  # Keep the label as it is\n        })\n    return augmented_sentences\n\n# Apply data augmentation on the training set\naugmented_train_data = augment_text(dataset)\n\n# Convert augmented data into Hugging Face Dataset format\naugmented_train_dataset = Dataset.from_pandas(pd.DataFrame(augmented_train_data))\n\n# Ensure the label column is of the same type (ClassLabel) as the original dataset\nlabel_feature = dataset['train'].features['label']  # Get original label feature type (ClassLabel)\naugmented_train_dataset = augmented_train_dataset.cast_column('label', label_feature)  # Cast labels\n\n# Concatenate original training data with augmented data using concatenate_datasets\ntrain_dataset_combined = concatenate_datasets([dataset['train'], augmented_train_dataset])\n\n# Load the tokenizer and model (using LastBERT)\ntokenizer = AutoTokenizer.from_pretrained(\"Peraboom/LastBERT\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Peraboom/LastBERT\", num_labels=2)\n\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples['sentence'], padding='max_length', truncation=True)\n\ntokenized_train_dataset = train_dataset_combined.map(tokenize_function, batched=True)\ntokenized_validation_dataset = dataset['validation'].map(tokenize_function, batched=True)\n\ntokenized_train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\ntokenized_validation_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n\n# Define the weighted loss function\nloss_fct = nn.CrossEntropyLoss(weight=class_weights)\n\n# Define the metrics for evaluation\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(-1)\n    \n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions)\n    precision = precision_score(labels, predictions)\n    recall = recall_score(labels, predictions)\n    mcc = matthews_corrcoef(labels, predictions)\n    \n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall,\n        'mcc': mcc\n    }\n\n# Custom Trainer to incorporate weighted loss\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get(\"labels\").to('cuda')\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        loss = loss_fct(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n# Set training arguments with hyperparameter tuning and early stopping\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=3e-5,  # Reduced learning rate for better fine-tuning\n    per_device_train_batch_size=8,  # Smaller batch size\n    per_device_eval_batch_size=16,\n    num_train_epochs=5,  # Train for fewer epochs with early stopping\n    weight_decay=0.01,\n    gradient_accumulation_steps=2,  # Gradient accumulation to handle smaller batches\n    logging_dir='./logs',\n    logging_steps=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model='mcc',  # Track MCC for model evaluation\n    greater_is_better=True\n)\n\n# Initialize the CustomTrainer with Early Stopping\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_validation_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Early stopping to prevent overfitting\n)\n\n# Train the model\ntrainer.train()\n\n# Evaluate the model at the end\neval_results = trainer.evaluate()\nprint(f\"Final Evaluation results: {eval_results}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T05:47:12.505726Z","iopub.execute_input":"2024-10-17T05:47:12.506093Z","iopub.status.idle":"2024-10-17T06:06:26.215119Z","shell.execute_reply.started":"2024-10-17T05:47:12.506059Z","shell.execute_reply":"2024-10-17T06:06:26.214182Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/8551 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d546dc468be43f6ab58cf87227e2629"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/17102 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2305556a6e7417ab6ffed282e70f9e1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5345' max='5345' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5345/5345 18:59, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Mcc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.667300</td>\n      <td>0.710126</td>\n      <td>0.524449</td>\n      <td>0.599354</td>\n      <td>0.717602</td>\n      <td>0.514563</td>\n      <td>0.056498</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.521400</td>\n      <td>0.769356</td>\n      <td>0.561841</td>\n      <td>0.641569</td>\n      <td>0.738267</td>\n      <td>0.567268</td>\n      <td>0.108271</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.525900</td>\n      <td>0.870177</td>\n      <td>0.598274</td>\n      <td>0.692590</td>\n      <td>0.735202</td>\n      <td>0.654646</td>\n      <td>0.120315</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.355800</td>\n      <td>1.017085</td>\n      <td>0.619367</td>\n      <td>0.717035</td>\n      <td>0.737537</td>\n      <td>0.697642</td>\n      <td>0.137641</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.329400</td>\n      <td>1.060022</td>\n      <td>0.626079</td>\n      <td>0.723404</td>\n      <td>0.740203</td>\n      <td>0.707351</td>\n      <td>0.147759</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [66/66 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Final Evaluation results: {'eval_loss': 1.0600216388702393, 'eval_accuracy': 0.62607861936721, 'eval_f1': 0.723404255319149, 'eval_precision': 0.7402031930333817, 'eval_recall': 0.7073509015256588, 'eval_mcc': 0.14775915812622145, 'eval_runtime': 3.7953, 'eval_samples_per_second': 274.816, 'eval_steps_per_second': 17.39, 'epoch': 5.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import required libraries\nimport random\nimport pandas as pd\nfrom datasets import load_dataset, Dataset, concatenate_datasets, ClassLabel\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef\nfrom torch import nn\nimport torch\n\n# Load the CoLA dataset\ndataset = load_dataset('glue', 'cola')\n\n# Class weighting: Calculate weights based on class distribution\ndef get_class_weights(dataset):\n    labels = [sample['label'] for sample in dataset['train']]\n    class_0_count = labels.count(0)\n    class_1_count = labels.count(1)\n    \n    # Assign higher weight to the minority class (unacceptable sentences)\n    total_samples = len(labels)\n    class_weights = [total_samples / class_0_count, total_samples / class_1_count]\n    return torch.tensor(class_weights).to('cuda')\n\nclass_weights = get_class_weights(dataset)\n\n# Simple Synonym Replacement using a dictionary (without external libraries)\nsynonym_dict = {\n    \"good\": [\"great\", \"excellent\", \"fine\"],\n    \"bad\": [\"terrible\", \"awful\", \"poor\"],\n    \"happy\": [\"joyful\", \"content\", \"pleased\"],\n    \"sad\": [\"unhappy\", \"sorrowful\", \"down\"],\n    # Add more synonym mappings here for real-world cases\n}\n\ndef replace_with_synonym(word):\n    if word in synonym_dict:\n        return random.choice(synonym_dict[word])\n    return word\n\ndef augment_text(dataset):\n    augmented_sentences = []\n    for sample in dataset['train']:\n        words = sample['sentence'].split()\n        augmented_words = [replace_with_synonym(word) for word in words]\n        augmented_sentence = \" \".join(augmented_words)\n        augmented_sentences.append({\n            'sentence': augmented_sentence,\n            'label': sample['label']  # Keep the label as it is\n        })\n    return augmented_sentences\n\n# Apply data augmentation on the training set\naugmented_train_data = augment_text(dataset)\n\n# Convert augmented data into Hugging Face Dataset format\naugmented_train_dataset = Dataset.from_pandas(pd.DataFrame(augmented_train_data))\n\n# Ensure the label column is of the same type (ClassLabel) as the original dataset\nlabel_feature = dataset['train'].features['label']  # Get original label feature type (ClassLabel)\naugmented_train_dataset = augmented_train_dataset.cast_column('label', label_feature)  # Cast labels\n\n# Concatenate original training data with augmented data using concatenate_datasets\ntrain_dataset_combined = concatenate_datasets([dataset['train'], augmented_train_dataset])\n\n# Load the tokenizer and model (using LastBERT)\ntokenizer = AutoTokenizer.from_pretrained(\"Peraboom/LastBERT\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Peraboom/LastBERT\", num_labels=2)\n\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples['sentence'], padding='max_length', truncation=True)\n\ntokenized_train_dataset = train_dataset_combined.map(tokenize_function, batched=True)\ntokenized_validation_dataset = dataset['validation'].map(tokenize_function, batched=True)\n\ntokenized_train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\ntokenized_validation_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n\n# Define the weighted loss function\nloss_fct = nn.CrossEntropyLoss(weight=class_weights)\n\n# Define the metrics for evaluation\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(-1)\n    \n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions)\n    precision = precision_score(labels, predictions)\n    recall = recall_score(labels, predictions)\n    mcc = matthews_corrcoef(labels, predictions)\n    \n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall,\n        'mcc': mcc\n    }\n\n# Custom Trainer to incorporate weighted loss\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get(\"labels\").to('cuda')\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        loss = loss_fct(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n# Set training arguments with hyperparameter tuning and early stopping\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=1e-5,  # Reduced learning rate for better fine-tuning\n    per_device_train_batch_size=8,  # Smaller batch size\n    per_device_eval_batch_size=16,\n    num_train_epochs=15,  # Train for fewer epochs with early stopping\n    weight_decay=0.01,\n    gradient_accumulation_steps=2,  # Gradient accumulation to handle smaller batches\n    logging_dir='./logs',\n    logging_steps=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model='mcc',  # Track MCC for model evaluation\n    greater_is_better=True\n)\n\n# Initialize the CustomTrainer with Early Stopping\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_validation_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Early stopping to prevent overfitting\n)\n\n# Train the model\ntrainer.train()\n\n# Evaluate the model at the end\neval_results = trainer.evaluate()\nprint(f\"Final Evaluation results: {eval_results}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T06:12:15.540202Z","iopub.execute_input":"2024-10-17T06:12:15.540767Z","iopub.status.idle":"2024-10-17T06:39:10.288706Z","shell.execute_reply.started":"2024-10-17T06:12:15.540600Z","shell.execute_reply":"2024-10-17T06:39:10.287743Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/8551 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34303e04845241f894b01e95a248c020"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7483' max='16035' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 7483/16035 26:46 < 30:36, 4.66 it/s, Epoch 7/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Mcc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.666900</td>\n      <td>0.711271</td>\n      <td>0.538830</td>\n      <td>0.619161</td>\n      <td>0.721402</td>\n      <td>0.542302</td>\n      <td>0.067831</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.515600</td>\n      <td>0.780203</td>\n      <td>0.560882</td>\n      <td>0.632424</td>\n      <td>0.750476</td>\n      <td>0.546463</td>\n      <td>0.129013</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.527700</td>\n      <td>0.889648</td>\n      <td>0.604027</td>\n      <td>0.703518</td>\n      <td>0.729167</td>\n      <td>0.679612</td>\n      <td>0.110390</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.315400</td>\n      <td>1.072131</td>\n      <td>0.620326</td>\n      <td>0.718750</td>\n      <td>0.736536</td>\n      <td>0.701803</td>\n      <td>0.136101</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.276300</td>\n      <td>1.209648</td>\n      <td>0.608821</td>\n      <td>0.702624</td>\n      <td>0.740399</td>\n      <td>0.668516</td>\n      <td>0.137035</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.236400</td>\n      <td>1.512874</td>\n      <td>0.636625</td>\n      <td>0.744437</td>\n      <td>0.724409</td>\n      <td>0.765603</td>\n      <td>0.118111</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.364600</td>\n      <td>1.497835</td>\n      <td>0.619367</td>\n      <td>0.718639</td>\n      <td>0.734783</td>\n      <td>0.703190</td>\n      <td>0.131671</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [66/66 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Final Evaluation results: {'eval_loss': 1.2096484899520874, 'eval_accuracy': 0.6088207094918504, 'eval_f1': 0.7026239067055393, 'eval_precision': 0.7403993855606759, 'eval_recall': 0.6685159500693482, 'eval_mcc': 0.13703510881390726, 'eval_runtime': 3.7977, 'eval_samples_per_second': 274.637, 'eval_steps_per_second': 17.379, 'epoch': 7.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import required libraries\nimport random\nimport pandas as pd\nfrom datasets import load_dataset, Dataset, concatenate_datasets, ClassLabel\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\nfrom transformers import AutoModelForSeq2SeqLM  # For back-translation\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef\nfrom torch import nn\nimport torch\n\n# Load the CoLA dataset\ndataset = load_dataset('glue', 'cola')\n\n# Class weighting: Calculate weights based on class distribution\ndef get_class_weights(dataset):\n    labels = [sample['label'] for sample in dataset['train']]\n    class_0_count = labels.count(0)\n    class_1_count = labels.count(1)\n    \n    # Assign higher weight to the minority class (unacceptable sentences)\n    total_samples = len(labels)\n    class_weights = [total_samples / class_0_count, total_samples / class_1_count]\n    return torch.tensor(class_weights).to('cuda')\n\nclass_weights = get_class_weights(dataset)\n\n# Load translation models for back-translation (English to French and back)\ntokenizer_fr_en = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")\nmodel_fr_en = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")\n\ntokenizer_en_fr = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\nmodel_en_fr = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n\n# Function for back-translation (English -> French -> English)\ndef back_translate(sentence):\n    # Translate from English to French\n    inputs = tokenizer_en_fr(sentence, return_tensors=\"pt\", max_length=512, truncation=True)\n    outputs = model_en_fr.generate(**inputs)\n    translated_fr = tokenizer_en_fr.decode(outputs[0], skip_special_tokens=True)\n    \n    # Translate back from French to English\n    inputs = tokenizer_fr_en(translated_fr, return_tensors=\"pt\", max_length=512, truncation=True)\n    outputs = model_fr_en.generate(**inputs)\n    back_translated = tokenizer_fr_en.decode(outputs[0], skip_special_tokens=True)\n    \n    return back_translated\n\n# Apply back-translation on the training set\ndef augment_text_with_back_translation(dataset):\n    augmented_sentences = []\n    for sample in dataset['train']:\n        back_translated_sentence = back_translate(sample['sentence'])\n        augmented_sentences.append({\n            'sentence': back_translated_sentence,\n            'label': sample['label']\n        })\n    return augmented_sentences\n\n# Apply data augmentation using back-translation\naugmented_train_data = augment_text_with_back_translation(dataset)\n\n# Convert augmented data into Hugging Face Dataset format\naugmented_train_dataset = Dataset.from_pandas(pd.DataFrame(augmented_train_data))\n\n# Ensure the label column is of the same type (ClassLabel) as the original dataset\nlabel_feature = dataset['train'].features['label']  # Get original label feature type (ClassLabel)\naugmented_train_dataset = augmented_train_dataset.cast_column('label', label_feature)\n\n# Concatenate original training data with augmented data using concatenate_datasets\ntrain_dataset_combined = concatenate_datasets([dataset['train'], augmented_train_dataset])\n\n# Load LastBERT tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"Peraboom/LastBERT\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Peraboom/LastBERT\", num_labels=2)\n\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples['sentence'], padding='max_length', truncation=True)\n\ntokenized_train_dataset = train_dataset_combined.map(tokenize_function, batched=True)\ntokenized_validation_dataset = dataset['validation'].map(tokenize_function, batched=True)\n\ntokenized_train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\ntokenized_validation_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n\n# Define the weighted loss function\nloss_fct = nn.CrossEntropyLoss(weight=class_weights)\n\n# Define the metrics for evaluation\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(-1)\n    \n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions)\n    precision = precision_score(labels, predictions)\n    recall = recall_score(labels, predictions)\n    mcc = matthews_corrcoef(labels, predictions)\n    \n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall,\n        'mcc': mcc\n    }\n\n# Custom Trainer to incorporate weighted loss\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get(\"labels\").to('cuda')\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        loss = loss_fct(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n# Set training arguments with hyperparameter tuning and dropout regularization\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=1e-5,  # Lower learning rate to fine-tune carefully\n    per_device_train_batch_size=8,  # Reduced batch size\n    per_device_eval_batch_size=16,\n    num_train_epochs=7,  # Train for more epochs\n    weight_decay=0.01,\n    gradient_accumulation_steps=2,  # Use gradient accumulation to match larger batches\n    max_grad_norm=1.0,  # Gradient clipping\n    logging_dir='./logs',\n    logging_steps=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model='mcc',  # Track MCC for model evaluation\n    greater_is_better=True\n)\n\n# Initialize the CustomTrainer with Early Stopping\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_validation_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Early stopping after 3 epochs of no improvement\n)\n\n# Train the model\ntrainer.train()\n\n# Evaluate the model at the end\neval_results = trainer.evaluate()\nprint(f\"Final Evaluation results: {eval_results}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T16:48:52.197129Z","iopub.execute_input":"2024-10-17T16:48:52.197555Z","iopub.status.idle":"2024-10-17T19:10:11.651233Z","shell.execute_reply.started":"2024-10-17T16:48:52.197515Z","shell.execute_reply":"2024-10-17T19:10:11.650339Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f5ca20fa4224a66ac479a73cb279ead"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61c1c8abcb074acd9e46d20e6f9e322e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab627d2992114bdab26376c1eef131ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb7e86d0276c4abeb86ef9103a461a5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe41e93f659c4fdf88cf95b84456e75e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cab275dfc234fecac572af50b4c49be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7187b0e3ec6a41c89035d347a11a2f46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"136fb6b944c44c208814c58e7d461421"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4f5eaec90474d73b91882b84492452f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e73691ff88c4b10a81128c4ea59b12f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e02c4478e154112b819db31f63748e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"329f3fdb9c734c4d85c73f06c0a31cc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3712f4557aa4a18bd464e473bfc7aa3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9928d8fca03e45c19422e473cdeb67dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/8551 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8ec2da0cee94fd69077fea091715104"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/17102 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50d435b0057449699665f057c0178ae9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7483' max='7483' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7483/7483 27:01, Epoch 7/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Mcc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.670900</td>\n      <td>0.684959</td>\n      <td>0.558965</td>\n      <td>0.642857</td>\n      <td>0.730159</td>\n      <td>0.574202</td>\n      <td>0.091863</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.642600</td>\n      <td>0.706842</td>\n      <td>0.536913</td>\n      <td>0.606357</td>\n      <td>0.735178</td>\n      <td>0.515950</td>\n      <td>0.092250</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.613500</td>\n      <td>0.726187</td>\n      <td>0.553212</td>\n      <td>0.621138</td>\n      <td>0.750491</td>\n      <td>0.529820</td>\n      <td>0.125146</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.577400</td>\n      <td>0.765058</td>\n      <td>0.594439</td>\n      <td>0.675862</td>\n      <td>0.755137</td>\n      <td>0.611650</td>\n      <td>0.155930</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.582400</td>\n      <td>0.778956</td>\n      <td>0.606903</td>\n      <td>0.693572</td>\n      <td>0.752026</td>\n      <td>0.643551</td>\n      <td>0.158263</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.480300</td>\n      <td>0.809651</td>\n      <td>0.634708</td>\n      <td>0.728826</td>\n      <td>0.748538</td>\n      <td>0.710125</td>\n      <td>0.171097</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.542500</td>\n      <td>0.810686</td>\n      <td>0.619367</td>\n      <td>0.708731</td>\n      <td>0.752336</td>\n      <td>0.669903</td>\n      <td>0.167244</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [66/66 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Final Evaluation results: {'eval_loss': 0.8096513152122498, 'eval_accuracy': 0.6347075743048898, 'eval_f1': 0.7288256227758007, 'eval_precision': 0.7485380116959064, 'eval_recall': 0.710124826629681, 'eval_mcc': 0.1710970335472794, 'eval_runtime': 3.703, 'eval_samples_per_second': 281.663, 'eval_steps_per_second': 17.823, 'epoch': 7.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Define the evaluation metrics function\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(-1)\n    \n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions)\n    precision = precision_score(labels, predictions)\n    recall = recall_score(labels, predictions)\n    mcc = matthews_corrcoef(labels, predictions)\n    \n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall,\n        'mcc': mcc\n    }\n\n# Assuming 'trainer' object is already defined, resume training\n# If not, load the tokenized datasets and use the existing model in memory\n# Load the previously tokenized datasets if needed (assuming they are still in memory)\n# tokenized_train_dataset = your previously tokenized dataset\n# tokenized_validation_dataset = your previously tokenized dataset\n\n# Set training arguments to resume training\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=1e-5,  # Lower learning rate to fine-tune carefully\n    per_device_train_batch_size=8,  # Reduced batch size\n    per_device_eval_batch_size=16,\n    num_train_epochs=20,  # Continue training for remaining epochs\n    weight_decay=0.01,\n    gradient_accumulation_steps=2,\n    max_grad_norm=1.0,  # Gradient clipping\n    logging_dir='./logs',\n    logging_steps=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model='mcc',  # Track MCC for model evaluation\n    greater_is_better=True\n)\n\n# Reuse the model already loaded in memory, and the tokenized datasets\n\n# Define the Trainer with the loaded model and datasets\ntrainer = Trainer(\n    model=model,  # Already loaded model\n    args=training_args,\n    train_dataset=tokenized_train_dataset,  # Pre-tokenized dataset\n    eval_dataset=tokenized_validation_dataset,  # Pre-tokenized dataset\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Early stopping after 3 epochs of no improvement\n)\n\n# Resume training\ntrainer.train()\n\n# Evaluate the model\neval_results = trainer.evaluate()\nprint(f\"Final Evaluation results: {eval_results}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T19:11:08.959661Z","iopub.execute_input":"2024-10-17T19:11:08.960075Z","iopub.status.idle":"2024-10-17T19:26:46.719738Z","shell.execute_reply.started":"2024-10-17T19:11:08.960037Z","shell.execute_reply":"2024-10-17T19:26:46.718858Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4276' max='21380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 4276/21380 15:33 < 1:02:14, 4.58 it/s, Epoch 4/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Mcc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.473500</td>\n      <td>0.738522</td>\n      <td>0.634708</td>\n      <td>0.732256</td>\n      <td>0.742165</td>\n      <td>0.722607</td>\n      <td>0.158057</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.411500</td>\n      <td>0.780911</td>\n      <td>0.626079</td>\n      <td>0.724576</td>\n      <td>0.738129</td>\n      <td>0.711512</td>\n      <td>0.143331</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.476000</td>\n      <td>0.765784</td>\n      <td>0.641419</td>\n      <td>0.750999</td>\n      <td>0.722151</td>\n      <td>0.782247</td>\n      <td>0.115394</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.396500</td>\n      <td>0.823179</td>\n      <td>0.659636</td>\n      <td>0.766294</td>\n      <td>0.729323</td>\n      <td>0.807212</td>\n      <td>0.148642</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [66/66 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Final Evaluation results: {'eval_loss': 0.7385215759277344, 'eval_accuracy': 0.6347075743048898, 'eval_f1': 0.7322557976106817, 'eval_precision': 0.7421652421652422, 'eval_recall': 0.7226074895977809, 'eval_mcc': 0.1580568026941382, 'eval_runtime': 3.7302, 'eval_samples_per_second': 279.613, 'eval_steps_per_second': 17.694, 'epoch': 4.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback, AutoConfig\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef\nfrom torch import nn\n\n# Assuming the augmented and tokenized datasets are already done previously and stored in variables\n# tokenized_train_dataset and tokenized_validation_dataset are assumed to be pre-loaded from previous augmentation.\n\n# Define Focal Loss class for better MCC\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.75, gamma=2, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        BCE_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n        pt = torch.exp(-BCE_loss)  # Prevents nans when probability is 0\n        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n\n        if self.reduction == 'mean':\n            return torch.mean(F_loss)\n        elif self.reduction == 'sum':\n            return torch.sum(F_loss)\n        else:\n            return F_loss\n\n# Define the evaluation metrics function to track MCC\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(-1)\n    \n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions)\n    precision = precision_score(labels, predictions)\n    recall = recall_score(labels, predictions)\n    mcc = matthews_corrcoef(labels, predictions)\n    \n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall,\n        'mcc': mcc  # Focus on MCC\n    }\n\n# Load the LastBERT model with dropout modification\nconfig = AutoConfig.from_pretrained(\"Peraboom/LastBERT\", hidden_dropout_prob=0.4)  # Increased Dropout for regularization\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Peraboom/LastBERT\", config=config)\n\n# Freeze layers for gradual fine-tuning\nfor param in model.bert.encoder.layer[:8].parameters():\n    param.requires_grad = False  # Freeze first 8 layers\n\n# Custom Trainer to incorporate Focal Loss and better MCC tracking\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get(\"labels\").to('cuda')\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        loss_fct = FocalLoss(alpha=0.75, gamma=2)  # Focal Loss with tuned alpha and gamma\n        loss = loss_fct(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n# Set training arguments with hyperparameter tuning and learning rate scheduler\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=1e-5,  # Lower learning rate to fine-tune carefully\n    lr_scheduler_type=\"cosine\",  # Cosine learning rate scheduler\n    per_device_train_batch_size=8,  # Reduced batch size\n    per_device_eval_batch_size=16,\n    num_train_epochs=15,  # Train for more epochs\n    weight_decay=0.01,\n    gradient_accumulation_steps=2,  # Use gradient accumulation to match larger batches\n    max_grad_norm=1.0,  # Gradient clipping\n    logging_dir='./logs',\n    logging_steps=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model='mcc',  # Focus on MCC for model evaluation\n    greater_is_better=True\n)\n\n# Initialize the CustomTrainer with early stopping\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,  # Use pre-tokenized and augmented dataset\n    eval_dataset=tokenized_validation_dataset,  # Use pre-tokenized validation dataset\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Early stopping after 3 epochs of no improvement\n)\n\n# Train the model\ntrainer.train()\n\n# Unfreeze the earlier frozen layers after some initial epochs\nfor param in model.bert.encoder.layer[:8].parameters():\n    param.requires_grad = True  # Unfreeze first 8 layers after initial training\n\n# Re-train the model after unfreezing\ntrainer.train()\n\n# Evaluate the model\neval_results = trainer.evaluate()\nprint(f\"Final Evaluation results: {eval_results}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T20:06:12.286863Z","iopub.execute_input":"2024-10-17T20:06:12.287595Z","iopub.status.idle":"2024-10-17T20:33:48.878469Z","shell.execute_reply.started":"2024-10-17T20:06:12.287552Z","shell.execute_reply":"2024-10-17T20:33:48.877582Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4276' max='16035' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 4276/16035 12:10 < 33:28, 5.85 it/s, Epoch 4/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Mcc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.117100</td>\n      <td>0.117719</td>\n      <td>0.691275</td>\n      <td>0.817460</td>\n      <td>0.691275</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.119000</td>\n      <td>0.118234</td>\n      <td>0.691275</td>\n      <td>0.817460</td>\n      <td>0.691275</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.126800</td>\n      <td>0.117263</td>\n      <td>0.691275</td>\n      <td>0.817460</td>\n      <td>0.691275</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.119100</td>\n      <td>0.117291</td>\n      <td>0.691275</td>\n      <td>0.817460</td>\n      <td>0.691275</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4276' max='16035' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 4276/16035 15:21 < 42:14, 4.64 it/s, Epoch 4/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Mcc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.116800</td>\n      <td>0.117804</td>\n      <td>0.691275</td>\n      <td>0.817460</td>\n      <td>0.691275</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.119700</td>\n      <td>0.117843</td>\n      <td>0.691275</td>\n      <td>0.817460</td>\n      <td>0.691275</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.125300</td>\n      <td>0.117083</td>\n      <td>0.691275</td>\n      <td>0.817460</td>\n      <td>0.691275</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.119600</td>\n      <td>0.116951</td>\n      <td>0.691275</td>\n      <td>0.817460</td>\n      <td>0.691275</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [66/66 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Final Evaluation results: {'eval_loss': 0.11780429631471634, 'eval_accuracy': 0.6912751677852349, 'eval_f1': 0.8174603174603174, 'eval_precision': 0.6912751677852349, 'eval_recall': 1.0, 'eval_mcc': 0.0, 'eval_runtime': 3.6987, 'eval_samples_per_second': 281.991, 'eval_steps_per_second': 17.844, 'epoch': 4.0}\n","output_type":"stream"}]}]}